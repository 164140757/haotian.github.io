<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin SEO -->





<title>Haotian Bai（白皓天） - Homepage</title>







<meta property="og:locale" content="en">
<meta property="og:site_name" content="Haotian Bai（白皓天）">
<meta property="og:title" content="Haotian Bai（白皓天）">


  <link rel="canonical" href="https://github.com/pages/hbai98/haotianbai.github.io/">
  <meta property="og:url" content="https://github.com/pages/hbai98/haotianbai.github.io/">



  <meta property="og:description" content="Stay hungry. Stay foolish. Yesterday is history, tomorrow is a mystery, today is a gift of God, which is why we call it the present.">







  <meta name="google-site-verification" content="fqiBEk7N4kdw0q4mpTVbu0EbTT1S0mh0sE-8fpcmwP8" />



<!-- end SEO -->


<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="assets/css/main.css">

<meta http-equiv="cleartype" content="on">
<head>
  <base target="_blank">
</head>
    <link rel="apple-touch-icon" sizes="180x180" href="images/haotian.png">
<link rel="icon" type="image/png" sizes="32x32" href="images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="images/favicon-16x16.png">
<link rel="manifest" href="images/site.webmanifest">

<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    <div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg masthead__menu-home-item"><a href="#about-me">Homepage</a></li>
          
            <li class="masthead__menu-item"><a href="/#about-me">About Me</a></li>
          
            <li class="masthead__menu-item"><a href="/#-news">News</a></li>
          
            <li class="masthead__menu-item"><a href="/#-publications">Publications</a></li>
          
            <li class="masthead__menu-item"><a href="/#-honors-and-awards">Honors and Awards</a></li>
          
            <li class="masthead__menu-item"><a href="/#-educations">Educations</a></li>
          
            <li class="masthead__menu-item"><a href="/#-invited-talks">Invited Talks</a></li>
          
            <li class="masthead__menu-item"><a href="/#-internships">Internships</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    <div id="main" role="main">
      
  <div class="sidebar sticky">
  

<div itemscope itemtype="http://schema.org/Person" class="profile_box">

  <div class="author__avatar">
    <img src="images/android-chrome-512x512.png" class="author__avatar" alt="Haotian Bai">
  </div>

  <div class="author__content">
    <h3 class="author__name">Haotian Bai</h3>
    <p class="author__bio">PhD student at AI Thrust of HKUST (GZ)</p>
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">More Info & Contact</button> -->
    <ul class="author__urls social-icons">
      
        <li><div style="white-space: normal; margin-bottom: 1em;">Stay hungry. Stay foolish. Yesterday is history, tomorrow is a mystery, today is a gift of God, which is why we call it the present. </div></li>
      
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Guangzhou, China</li>
      
      
      
      
        <li><a href="mailto:hbai965@connect.hkust-gz.edu.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i> Email</a></li>
      
      
       
      
        <li><a href="https://twitter.com/HaotianBai1"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
      
      
        <li><a href="https://www.facebook.com/haotian.bai.14"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i> Facebook</a></li>
      
      
      
        <li><a href="https://www.linkedin.com/in/haotian-bai-17373318b"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
      
      
      
        <li><a href="https://github.com/hbai98"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com/citations?user=DIy4cA0AAAAJ"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
      <div class="author__urls_sm">
      
      
        <a href="mailto:hbai965@connect.hkust-gz.edu.cn"><i class="fas fa-fw fa-envelope" aria-hidden="true"></i></a>
      
      
       
      
        <a href="https://twitter.com/HaotianBai1"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i></a>
      
      
        <a href="https://www.facebook.com/haotian.bai.14"><i class="fab fa-fw fa-facebook-square" aria-hidden="true"></i></a>
      
      
      
        <a href="https://www.linkedin.com/in/haotian-bai-17373318b"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
        <a href="https://github.com/hbai98"><i class="fab fa-fw fa-github" aria-hidden="true"></i></a>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <a href="https://scholar.google.com/citations?user=DIy4cA0AAAAJ"><i class="fas fa-fw fa-graduation-cap"></i></a>
      
      
      
      
      
    </div>
  </div>
</div>

  
  </div>

    
      <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
        <meta itemprop="headline" content="">
        <div class="page__inner-wrap">
          <section class="page__content" itemprop="text">
            <p><span class="anchor" id="about-me"></span></p>

<p>Hello, my name is Haotian Bai(白皓天). I am currently pursuing a Ph.D. in computer vision, AI Thrust, at HKUST’s Guangzhou campus, supervised by Prof. <a href="https://www.hkust-gz.edu.cn/people/hui-xiong/">Hui Xiong(熊辉)</a>.</p>

<p>My primary research interests are in 3D reconstruction, with a focus on Neural Radiance Field (NeRF), and its applications using multi-model data. I am also interested in robotics, specifically simultaneous localization and mapping (SLAM). 
<!-- Beyond that, my personal research interests include graph theory, complex systems, life science especially aging, physics(thermodynamics),and deep reinforcement learning (DRL). -->
My papers <a href="https://scholar.google.com/citations?user=DIy4cA0AAAAJ"><img src="https://img.shields.io/endpoint?logo=Google%20Scholar&amp;url=https%3A%2F%2Fcdn.jsdelivr.net%2Fgh%2Fhbai98%2Fhbai98.github.io@google-scholar-stats%2Fgs_data_shieldsio.json&amp;labelColor=f6f6f6&amp;color=9cf&amp;style=flat&amp;label=citations" /></a> are accepted at prestigious AI conferences such as CVPR, ICCV, ECCV, and NeurIPS.
I also serve as reviewers on well known journals including TPAMI, TOMM, and TIP.
Please feel free to comment on <a href="https://github.com/hbai98">Github <img src="https://img.shields.io/github/stars/hbai98?style=social" alt="" /></a> for further discussion on my works.</p>

<p>In my spare time, I enjoy cooking, playing basketball, listening to music, and learning about starting a business.</p>

<p>Recently,</p>

<ul>
  <li>I am investigating on 3D/4D scene reconstruction and generation.</li>
</ul>

<!-- - I am working with <a href='https://facultyprofiles.hkust-gz.edu.cn/faculty-personal-page/LI-Haoxiang/haoxiangli'>Prof.Haoxiang Li</a> on <a href='https://www.wikiwand.com/en/Photoemission_spectroscopy'>photoemission spectroscopy</a> for quantum materials.
- I am working with <a href='https://scholar.google.com/citations?user=G1NiRmwAAAAJ&hl=en'>Prof.Yize Chen</a> on reinforcement learning with Large Language Model(LLM). -->
<!-- - I am working with Bonan Liu and <a href='https://panhui.people.ust.hk/'>Prof. Pan Hui</a> on 3D vision and robotics. -->

<h1 id="-news">🔥 News</h1>
<ul>
  <li><em>2024.11</em>: I join the research team lead by <a href="https://www.hkust-gz.edu.cn/people/hui-xiong/">Prof.Hui Xiong (熊辉)</a>.</li>
  <li><em>2023.09</em>: I join AI Thrust, Info Hub in HKUST(GZ) as a Ph.D. student.</li>
  <li><em>2023.07</em>:  🎉🎉 One paper is accepted by ICCV2023.</li>
  <li><em>2023.03</em>:  🎉🎉 One paper is accepted by CVPR2023 highlight(top 2.5%).</li>
  <li><em>2022.07</em>:  🎉🎉 My first paper is accepted by ECCV2022.</li>
  <li><em>2022.05</em>: I join AI Thrust, Info Hub in HKUST(GZ) as a research assistant supervised by <a href="https://vlislab22.github.io/vlislab/linwang.html">Addison Lin Wang(王林)</a>.</li>
  <li><em>2021.07</em>: I join School of Data Science, CUHKSZ as a research assistant supervised by <a href="http://www.zhangruimao.site/">Ruimao Zhang(张瑞茂)</a>.</li>
  <li><em>2021.06</em>: I graduate from Shanghai Univeristy with my bachelor’s degree in computer science and engineering.</li>
  <li><em>2019.09</em>: I was selected as (1/300) Chinese undergraduate student representatives to attend the <a href="https://raeng.org.uk/about-us/international/international-partnerships/global-grand-challenges-summit-2019#:~:text=The%20Summit%20took%20place%20on,entrepreneurs%2C%20and%20policymakers%20in%20attendance.">Global Grand Challenge Summit</a> in London.</li>
  <li><em>2019.06</em>: I was selected as (1/30) undergraduate student representatives to join <a href="https://leadership.wharton.upenn.edu/">the leadership program</a> in The Wharton School of UPEEN.</li>
</ul>

<h1 id="-publications">📝 Publications</h1>
<div class="paper-box"><div class="paper-box-image"><div><div class="badge">Arxiv</div><img src="images/hi-neus.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2409.13158">High-Fidelity Mask-free Neural Surface Reconstruction for Virtual Reality</a></p>

    <p><strong>Haotian Bai</strong>, Yize Chen, Lin Wang</p>

    <p><a href="https://vlislab22.github.io/Hi-NeuS/"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="DIy4cA0AAAAJ:_FxGoFyzp5QC"></span></strong>| <a href="https://youtu.be/hrkM5N7AltY"><strong>Video</strong></a> | <a href="https://github.com/hbai98/Hi_NeuS"><img src="https://img.shields.io/github/stars/hbai98/Hi_NeuS?style=social" alt="" /></a></p>

    <ul>
      <li>A novel rendering-based framework for neural implicit surface reconstruction, aiming to recover compact and precise surfaces <strong>without multi-view object masks</strong>.</li>
      <li>Since overlaps in images implicitly identifies the surface that a user intends to capture, Hi-NeuS takes multi-view rendering weights to guide the signed distance functions of neural surfaces in a <strong>self-supervised</strong> manner.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">Arxiv</div><img src="images/componerf.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2303.13843">CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout</a></p>

    <p><strong>Haotian Bai</strong>, Yuanhuiyi Lyu, Lutao Jiang, 
Sijia Li, Haonan Lu, Xiaodong Lin, Lin Wang</p>

    <p><a href="https://vlislab22.github.io/componerf/"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="DIy4cA0AAAAJ:YsMSGLbcyi4C"></span></strong>| <a href="https://www.youtube.com/watch?v=eufdSsa-P9U"><strong>Video</strong></a> | <a href="https://github.com/hbai98/Componerf"><img src="https://img.shields.io/github/stars/hbai98/Componerf?style=social" alt="" /></a></p>

    <ul>
      <li>A novel framework that synthesizes coherent <strong>multi-object</strong> scenes by integrating textual descriptions with box-based spatial arrangements.</li>
      <li>CompoNeRF is designed for precision and adaptability, allowing for individual NeRFs, each denoted by a unique prompt color, to be <strong>composed</strong>, <strong>decomposed</strong>, and <strong>recomposed</strong> with ease, streamlining the construction of complex scenes from cached models after decomposition.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ICCV 2023</div><img src="images/DOT.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2307.15333">Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF</a></p>

    <p><strong>Haotian Bai</strong>, Yiqi Lin, Yize Chen, Lin Wang</p>

    <p><a href="https://vlislab22.github.io/DOT/"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="DIy4cA0AAAAJ:Y0pCki6q_DkC"></span></strong>| <a href="https://www.youtube.com/watch?v=i9MnoFhH8Ec"><strong>Video</strong></a> | <a href="https://github.com/hbai98/DOT"><img src="https://img.shields.io/github/stars/hbai98/DOT?style=social" alt="" /></a></p>

    <ul>
      <li>A more compact and fertile PlenOctree (POT) NeRF representation.</li>
      <li><strong>Inspiration</strong>: POT’s fixed structure for direct optimization is sub-optimal as the scene complexity evolves continuously with updates to cached color and density, necessitating refining the sampling distribution to capture signal complexity accordingly.</li>
      <li><strong>Competitive</strong>: DOT outperforms POT by enhancing visual quality, reducing over
55.15/68.84% parameters, and providing 1.7/1.9 times FPS for NeRF-synthetic and Tanks and Temples.</li>
    </ul>
  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">CVPR 2023</div><img src="images/PatchMix.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/abs/2303.13434">Patch-Mix Transformer  for  Unsupervised Domain Adaptation:  A Game Perspective</a></p>

    <p>Jinjing Zhu*, <strong>Haotian Bai<sup>*</sup></strong>, Lin Wang</p>

    <p><a href="https://vlis2022.github.io/cvpr23/PMTrans.html"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="DIy4cA0AAAAJ:UeHWp8X0CEIC"></span></strong>| <a href="https://www.youtube.com/watch?v=WNFlX0WFAO8"><strong>Video</strong></a> | <a href="https://github.com/JinjingZhu/PMTrans"><img src="https://img.shields.io/github/stars/jinjingZhu/PMTrans?style=social" alt="" /></a></p>

    <ul>
      <li>Be selected as one of CVPR <span style="color:red">(highlight)</span> papers(<strong>top 2.5%</strong>)</li>
      <li><strong>Large Domain Gap</strong>:  PMTrans bridges source and target domains with an intermediate domain in a relatively smooth way.</li>
      <li><strong>Game Theory</strong>: Interpret UDA as a min-max CE game with three players, including the feature extractor, classifier, and PatchMix to find the Nash Equilibria.</li>
      <li><strong>Competitive</strong>: PMTrans surpasses ViT-based and CNN-based SoTA methods by +3.6% on Office-Home, +1.4% on Office-31, and +17.7% on DomainNet.</li>
    </ul>

  </div>
</div>

<div class="paper-box"><div class="paper-box-image"><div><div class="badge">ECCV 2022</div><img src="images/SCM.png" alt="sym" width="100%" /></div></div>
<div class="paper-box-text">

    <p><a href="https://arxiv.org/pdf/2207.10447">Weakly Supervised Object Localization via Transformer with Implicit Spatial Calibration</a></p>

    <p><strong>Haotian Bai</strong>, Ruimao Zhang, Jiong Wang, Xiang Wan</p>

    <p><a href="https://github.com/hbai98/SCM"><strong>Project</strong></a> <strong><span class="show_paper_citations" data="DIy4cA0AAAAJ:W7OEmFMy1HYC"></span></strong>| <a href="https://www.youtube.com/watch?v=zQdUudmTPOQ"><strong>Video</strong></a> | <a href="https://github.com/hbai98/SCM"><img src="https://img.shields.io/github/stars/hbai98/SCM?style=social" alt="" /></a></p>

    <ul>
      <li>SCM is the external transformer based solution for Weakly Supervised Object Localization.</li>
      <li><strong>Lightweight</strong>: SCM is an external Transformer model that produces no additional parameters.</li>
      <li><strong>Competitive</strong>: SCM outperforms most competitive frameworks (CNN &amp; Transformer) using only about 𝟐𝟎%~𝟑𝟎% of their parameters.</li>
    </ul>

  </div>
</div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">NeurIPS 2022</code><span style="color:red">(Oral)</span> <a href="http://www.amos.sribd.cn/">AMOS: A large-scale abdominal multi-organ benchmark for versatile medical image segmentation.</a>, Yuanfeng Ji, <strong>Haotian Bai</strong>, Jie Yang, Chongjian Ge, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhang, Wanling Ma, Xiang Wan, Ping Luo. <strong><span class="show_paper_citations" data="DIy4cA0AAAAJ:u-x6o8ySG0sC"></span></strong></li>
</ul>

<h1 id="-honors-and-awards">🎖 Honors and Awards</h1>
<ul>
  <li><em>2024.08</em> Serve as a reviewer for Transactions on Multimedia Computing Communications and Applications (TOMM).</li>
  <li><em>2024.06</em> Serve as a reviewer for IEEE Transactions on Image Processing (TIP).</li>
  <li><em>2024.04</em> Serve as a reviewer for Transactions on Multimedia Computing Communications and Applications (TOMM).</li>
  <li><em>2024.03</em> Serve as a reviewer for IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).</li>
  <li><em>2019.07</em> The third municipal price in the national math modeling competition.</li>
  <li><em>2018.10</em> The third municipal prize in the 2019 National College Students Extracurricular Academic Practice
Competition(Alzheimer).</li>
  <li><em>2018.06</em> The third national prize in the China University program development competition.</li>
</ul>

<h1 id="-educations">📖 Educations</h1>

<ul>
  <li><em>2023.09 - (now)</em>, Ph.D., Hong Kong University of Science and Technology.</li>
  <li><em>2017.09 - 2021.06</em>, B.E., Shanghai University</li>
</ul>

<h1 id="-invited-talks">💬 Invited Talks</h1>

<ul>
  <li><em>2022.12</em>, The global Ph.D. talk for sharing papers accepted in ECCV 2022, <a href="https://www.aitime.cn/">AI TIME</a>, International Science and Technology Information Center, Tsinghua University. | <a href="https://www.bilibili.com/video/BV1c44y1D769">[video]</a></li>
</ul>

<h1 id="-internships">💻 Internships</h1>

<ul>
  <li><em>2022.05 - 2023.08</em>, research assistant at AI Thrust, Info Hub in HKUST(GZ).</li>
  <li><em>2021.07 - 2022.04</em>, research assistant at School of Data Science, CUHKSZ.</li>
</ul>


          </section>
        </div>
      </article>
    </div>

    <script src="assets/js/main.min.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-V7YQEJ4JVF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', "G-V7YQEJ4JVF");
</script>


<script>
    $(document).ready(function () {
        
        var gsDataBaseUrl = 'https://cdn.jsdelivr.net/gh/hbai98/haotianbai.github.io@'
        
        $.getJSON(gsDataBaseUrl + "google-scholar-stats/gs_data.json", function (data) {
            // var totalCitation = data['citedby']
            // document.getElementById('total_cit').innerHTML = totalCitation;
            var citationEles = document.getElementsByClassName('show_paper_citations')
            Array.prototype.forEach.call(citationEles, element => {
                var paperId = element.getAttribute('data')
                var numCitations = data['publications'][paperId]['num_citations']
                element.innerHTML = '| Citations: ' + numCitations;
            });
        });
    })
</script>

  </body>
</html>
